# feature engineering

3. scailing

대부분의 데이터가 범위들이 다르고 단위가 다르다. 단위와 범위가 다른 경우 데이터끼리 직접적인 비교가 불가능하다. 이런 문제를 해결하기 위해 scailing을 진행한다.

### normalization (정규화)

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/b1ad4853-98e7-47b1-a4f0-06ab03afa666)


데이터를 0과 1 사이의 값으로 변화하여 이사이의 영향력이 커지므로 정규화후 이상치를 다루는 것이 좋다. 

### standardization


![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/82f92653-fdca-4dcd-90a6-52c11ede5aa1)

값을디 정규분포를 따른다고 가정하고 값들을 0의 평균 1의 표준편차를 갖도록 변화해주는 것. 표준화를 해주면 정규화처럼 특성값의 범위가 0~1 사이로 균일하게 바뀌지는 않는다. 

# relu가 sigmooid보다 선호되는 이유

1. 계산 효율성

활성화 함수의 계산 효율성은 입력이 주어졌을 때 출력을 얼마나 빨리 계산할 수 있느냐다. 대규모 데이터셋이나 복잡한 모델을 다룰 때는 당연히 더 간단함 함수가 계산 효율성이 높다. 거런 측면에서 sigmoid는 지수 함수가 포함되기 때문에 나눗셈 같은 복잡한 수학적 연산이 필요하지만 relu는 간단하게 계산할 수 있기 때문에 계산 효율성을 높일 수 있고 이는 훈련 프로세스를 더 빠르게 만들고 최적의 값으로 수렴하는 속도를 높여줄 수 있다. 

2. gradient 소실 방지

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/78e43830-c2f6-4d19-a04e-ea8300048a7c)


sigmoid 함수는 경사 소리 문제가 있다. 입력 값이 매우 크거나 작다면 기울기도 0에 가까워 진다. 기울기가 -에 가까워지면 가중치가 매우 느리게 업데이트되므로 학습 과정이 느려지고, 신경망이 수렴하기 어렵게 된다. local minimum에 갇히게 될 수 있다. 

relu는 양수 입력 값에 대해 일정한 기울기를 갖고 있으므로 이 문제를 방지 가능



[feature engineering](https://velog.io/@baeyuna97/Feature-engineering%EC%9D%B4%EB%9E%80)
