# feature engineering

3. scailing

대부분의 데이터가 범위들이 다르고 단위가 다르다. 단위와 범위가 다른 경우 데이터끼리 직접적인 비교가 불가능하다. 이런 문제를 해결하기 위해 scailing을 진행한다.

### normalization (정규화)

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/b1ad4853-98e7-47b1-a4f0-06ab03afa666)


데이터를 0과 1 사이의 값으로 변화하여 이사이의 영향력이 커지므로 정규화후 이상치를 다루는 것이 좋다. 

### standardization


![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/82f92653-fdca-4dcd-90a6-52c11ede5aa1)

값을디 정규분포를 따른다고 가정하고 값들을 0의 평균 1의 표준편차를 갖도록 변화해주는 것. 표준화를 해주면 정규화처럼 특성값의 범위가 0~1 사이로 균일하게 바뀌지는 않는다. 

# relu가 sigmooid보다 선호되는 이유

1. 계산 효율성

활성화 함수의 계산 효율성은 입력이 주어졌을 때 출력을 얼마나 빨리 계산할 수 있느냐다. 대규모 데이터셋이나 복잡한 모델을 다룰 때는 당연히 더 간단함 함수가 계산 효율성이 높다. 거런 측면에서 sigmoid는 지수 함수가 포함되기 때문에 나눗셈 같은 복잡한 수학적 연산이 필요하지만 relu는 간단하게 계산할 수 있기 때문에 계산 효율성을 높일 수 있고 이는 훈련 프로세스를 더 빠르게 만들고 최적의 값으로 수렴하는 속도를 높여줄 수 있다. 

2. gradient 소실 방지

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/78e43830-c2f6-4d19-a04e-ea8300048a7c)


sigmoid 함수는 경사 소리 문제가 있다. 입력 값이 매우 크거나 작다면 기울기도 0에 가까워 진다. 기울기가 -에 가까워지면 가중치가 매우 느리게 업데이트되므로 학습 과정이 느려지고, 신경망이 수렴하기 어렵게 된다. local minimum에 갇히게 될 수 있다. 

relu는 양수 입력 값에 대해 일정한 기울기를 갖고 있으므로 이 문제를 방지 가능

# Mean Squared Error (MSE)

추측값에 대한 정확성을 측정하는 방법. 평균과 제곱을 이용하여 오차를 계산한다. 

MSE가 0에 가까울 수록 추측값이 원본에 가까워지기 때문에 숫자가 작을 수록 좋다.

평균 값을 뺀 후에 제곱합을 하기 때문에 특이값이 존재하면 수치가 많이 늘어난다. 

작은 오류 OK

# Mean Absolute Error (MAE) 평균 절대 오차

MAE는 데이터 예측 모델의 성능을 평가하는 데 사용되는 일반적인 평가 지표 중 하나. 

실제값과 예측값 간의 차이를 평균하여 모델의 예측 정확성을 측정

MAE = (1/n) * Σ|실제값 - 예측값|

n : 데이터 샘플의 개수

Σ : 합

모든 예측값에 대해 절대 오차를 구하고 이를 데이터 샘플 수로 나누어 평균을 계산

따라서 MAE는 예측 오차의 절대값들의 평균

MAE가 낮을수록 모델의 예측 정확성이 높다고 볼 수 있다. 

큰 오류 OK

# 머신러닝의 학습 방법

1. 지도학습 : 답이 주어지는 경우. 가장 성능이 좋음. 라벨을 어떻게 붙이느냐가 중요.
   1) 분류 분석 : 문서, 이미지 분류 등
   2) 회기 분석 : 주식, 부동산 예측 등
  
2. 비지도 학습 : 라벨이 없는 데이터들에서 패턴을 찾음(ex. 군집)

3. 강화학습 : (ex. 알파고) 보상을 줌(+, -)

- overfitting : 모델이 **학습 데이터**에만 과도하게 최적화되어, 실제 예측을 다른 데이터로 수행할 경우에는 예측 성능이 과도하게 떨어지는 것을 의미

- 학습 데이터를 다시 분할 -> 학습 데이터 + 학습된 모델의 성능을 1차 평가하는 검증 데이터

- 검증 데이터 세트로 1차 평가를 한 뒤에, 최종적으로 테스트 데이터 세트에 적용해 평가하는 프로세스

- train / validaion / test dataset

### 교차 검증을 보다 간편하게 : cross_val_score()

- 교차 검증을 위한 API 제공

- cross_val_score(estimator, X_train, y_train, scoring = 평가지표, cv = 교차 검증 폴드 수)

- estimator: 분류 알괼즘(classifier) / 회귀(regression) 구분

- scoring : 예측 성능 평가 지표('accuracy', 'top_k_accuracy') 등등

- cv : fold 수

DecisionTreeClassifier는 분류를 위한 결정 트리 모델


# CNN

CNN 구조는 기존의 완전 연결 계층(Fully-Connected Layer)과 다르게 구성되어 있따. 

CNN은 convolutional layer과 pooling layer들을 활성화 함수 앞뒤에 배치하여 만들어진다. 

주어진 입려값 : 28 * 28 크기의 이미지

이 이미지를 대상으로 여러개의 필터(커널)을 사용하여 결과값(feature mapping)을 얻는다. 즉, 한개의 28*28 이미지 입력값에 10개의 5*5 필터를 사용해서 10개의 24*24 matrics, 즉 convolution의 결과값을 만들어냈다. 

이 결과값에 activation function을 적용한다. 

convolutional layer은 convolution 처리와 activation function으로 구성된다.

**A Convolutional Layer = convolution + activation**

### Activation function

선형함수(linear function)인 convolution에 비선형성(nonlinearity)를 추가하기 위해 사용

### 첫번째 pooling layer

**pooling** : 이 전 단계에서 convolution 과정을 통해 많은 수의 결과값(이미지)들을 생성해따. 하나의 이미지에서 10개의 이미지 결과값이 도출되어버리면 값이 너무 많아졌다는 것이 문제가 된다. 따라서 고안된 방법이 pooling이라는 과정이다. 

pooling은 각 결과값(feature map)의 dimentionality를 축소해 주는 것을 목적으로 둔다. 즉 correlation이 낮은 부분을 삭제(?)하여 각 결과값의 크기(dimension)을 줄이는 과정이다. 

### pooling 방법

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/0d49301f-2536-43d7-a210-8ee160d1100f)


1. max pooling

2. average pooling

### 두번째 convolutional layer

convolutional layer에서는 텐서 convolution을 적용

이전의 pooling layer에서 얻어낸 12*12*10 텐서(order-3 tensor)를 대상을 5x5x10 크기의 텐서 필터 20개를 사용해준다. 그렇게 되면 각각 8x8 크기를 가진 결과값 20개를 얻어낼 수 있다. 


### 두번째 pooling layer

두번째 pooling layer다. 

전과 똑같은 방식으로 pooling 처리 과정을 해주면 더 크기가 작아진 20개의 4*4 결과값을 얻는다. 

### flatten(vectorization)

그 이후 4*4*20 텐서를 일자 형태의 데이터로 쭉 펼친다. 

이 과정을 flatten 또는 vectorizatino이라고 한다. 

각 세로줄을 일렬로 쭉 세워두는 것.

이것은 320-dimension을 가진 vector 형태가 된다. 

##### 왜 1차원 데이터로 변형해도 상관 없을까요?

두 번째 pooling layer에서 얻어낸 4*4 크기의 이미지는 이미지 자체보다 입력된 데이터에서 얻어온 특이점ㅇ 데이터가 된다. 

즉 1차원의 벡터 데이터로 변형시켜주어도 무관한 상태가 된다는 의미.

### fully-connected layer(dense layers)

마지막에 하나 혹은 하나 이상의 fully connected layer를 적용시키고 마지막에 softmax activation function을 적용해주면 최종 결과물이 나온다. 


### 매개변수(parameter)와 hyper-매개변수

모델 매개변수(parameteR)는 모델 내부에 있으며 데이터로부터 값이 추정될 수 있는 설정 변수(configuration variable)이다 .

모델 하이퍼파라미터(hyper-parameter)는 모델 외부에 있으며 데이터로부터 값이 추정될 수 없는 설정변수다. 



[MAE](https://blog.naver.com/djsudaqw/223175628566)

[feature engineering](https://velog.io/@baeyuna97/Feature-engineering%EC%9D%B4%EB%9E%80)

[cross_val_score](https://blog.naver.com/yamyamsis/223070461474)

[cnn](https://mijeongban.medium.com/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-cnn-convolutional-neural-networks-%EC%89%BD%EA%B2%8C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-836869f88375)


