![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/899b7618-a58c-4c8f-be52-191304de595e)# feature engineering

3. scailing

대부분의 데이터가 범위들이 다르고 단위가 다르다. 단위와 범위가 다른 경우 데이터끼리 직접적인 비교가 불가능하다. 이런 문제를 해결하기 위해 scailing을 진행한다.

### normalization (정규화)

데이터를 0과 1 사이의 값으로 변화하여 이사이의 영향력이 커지므로 정규화후 이상치를 다루는 것이 좋다. 

# relu가 sigmooid보다 선호되는 이유

1. 계산 효율성

활성화 함수의 계산 효율성은 입력이 주어졌을 때 출력을 얼마나 빨리 계산할 수 있느냐다. 대규모 데이터셋이나 복잡한 모델을 다룰 때는 당연히 더 간단함 함수가 계산 효율성이 높다. 거런 측면에서 sigmoid는 지수 함수가 포함되기 때문에 나눗셈 같은 복잡한 수학적 연산이 필요하지만 relu는 간단하게 계산할 수 있기 때문에 계산 효율성을 높일 수 있고 이는 훈련 프로세스를 더 빠르게 만들고 최적의 값으로 수렴하는 속도를 높여줄 수 있다. 

2. gradient 소실 방지

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/78e43830-c2f6-4d19-a04e-ea8300048a7c)


sigmoid 함수는 경사 소리 문제가 있다. 입력 값이 매우 크거나 작다면 기울기도 0에 가까워 진다. 기울기가 -에 가까워지면 가중치가 매우 느리게 업데이트되므로 학습 과정이 느려지고, 신경망이 수렴하기 어렵게 된다. local minimum에 갇히게 될 수 있다. 

relu는 양수 입력 값에 대해 일정한 기울기를 갖고 있으므로 이 문제를 방지 가능

