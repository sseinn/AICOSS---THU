# Logistic Regression

사건(Event)이 발생할 가능성을 예측하는 데 사용하는 모델

특정 조건(환경)이 주어졌을 때, 사건이 일어나는지, 일어나지 않는지에 대해 다룸

예측은 0과 1로만 이루어짐

1 : 사건이 일어남

0 : 사건이 일어나지 않음

# Sigmoid Function

로지스틱 회귀 모델에 사용하는 활성화 함수는 시그모이드다. 

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/e8c8df95-b48d-4300-9fd8-d8103a2c64a3)

```
import numpy as np
import matplotlib.pyplot as plt

x = np.arange(-6, 6, 0.1)
phi = 1 / (1 + np.exp(-x))

plt.plot(x, phi)
plt.xlabel('x')
plt.ylabel('phi')

plt.show()
```

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/e28ec939-c08f-4144-9b4c-d5f5c8791218)


시그모이드 함수는 x값이 0일 경우 y값은 0.5라는 결과다. 

x가 0보다 크면 y는 0.5를 기준으로 긍정(사건이 발생) 결과가 나타나고 x가 0보다 작으면 y는 0.5를 기준으로 부정(사건이 발생하지 않는) 결과가 나타난ㄷ. 

y가 0.5인 경우 긍정이라고 보는 경우도 있고, 부저으로 보는 경우도 있다. 


# Linear Regression

선형회귀란 x와 y의 선형적인 관계를 구해 값을 예측하는 것

x는 변수로 어느 값이든 적용 가능

기울기 a와 절편 b는 모르는 상태

-> 기울기와 절편의 값을 알게 된다면, 원하는 x값을 대입했을 때 y값을 얻을 수 있단 말과 동일하다
(y = ax + b)

a : weight

b : bias

# Mean Squared Error - MSE (평균제곱오차)

선형회귀에서 가중치와 바이어스는 MSE를 최소화 하는 방향으로 구한다. 

평균제곱오차 : 데이터의 오차를 제곱해서 데이터의 개수로 나눈 값

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/48b5e5b4-b199-43a9-aa6e-5e0e43a5853a)

그래프에서 데이터의 값은 9이지만, 방정식의 값은 4여서 오차가 9 - 4 = 5여서 오차가 +5가 된다. 

그래프에서 데이터의 값은 3이지만, 방정식의 값은 5여서 오차가 3 - 5 = -2여서 오차가 -2가 된다. 

오차는 음수, 양수 둘다 가능하기 때문에 오차를 제곱하는 것이다. 

그리고 평균을 구하기 때문에 데이터의 개수로 오차의 합을 나눈다. 


# regularization

- 정규화

정규화란 모델 복잡도에 대한 일종의 패널티로, 오버피팅 예방, 일반화(Generalization) 성능을 높이는데 도움을 준다. Regularization 방법으로 L1, L2, Dropout, Early Stopping 등이 있다.

- batch normalization

- weight regularization

- dropout(학습 당시 랜덤으로 절반의 뉴런만 사용하기)

=> weight regularization에 해당

# L1 - Norm, L2 - Norm

모델에서 학습을 진행할 때 학습 데이터에 따라 특정 weight의 값이 커질 수 있다. 이렇게 되면 오버피팅이 일어날 가능성이 아주 높은데, 이를 방지하기 위해 L1, L2 정규화를 사용한다. 

L1 Norm : 벡터 p, q의 각 원소들의 차이의 절대값의 합

L2 Norm : 벡터 p, q의 유클리디안 거리(직선 거리)

# L1 Regularization

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/146bd698-0874-49b4-85d3-6b4045aeac88)


더하기 전 좌측이 일반적인 cost function이고 여기에 가중치 절대값을 더해준다. 

편미분을 하면 w값은 상수값이 되어버리고, 그 부호에 따라 +-가 결정된다. 

가중치가 너무 작은 경우는 상수 값에 의해 weight가 0이 된다. 

=> 결과적으로 몇몇 중요한 가중치들만 남게 됨


# L2 Regularization

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/39f7a6f3-276e-4f6b-a8fe-a5367f56db3c)

Cost function에 제곱한 가중치 값을 더해줌으로써 편미분을 통해 역전파를 할 때 Cost뿐만 아니라 가중치 또한 줄어드는 방식으로 학습한다. 특정 가중치가 비이상적으로 커지는 상황을 방지하고, weight가 decay 가능하다. 

=> 전체적으로 가중치를 작게하여 과적합을 방지

# Decision Trees

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/8b98ece7-dce2-4312-b1a5-438bb173d784)


root note : 가장 위에 있는 노드(work to do) / 모든 데이터를 포함하고 있다 

inner node 

leaf node : (stay in) 가장 마지막 노드

노드는 top -> bottom으로 구성됨

- 의사결정트리는 일련의 분류 규칙을 통해 데이터를 분류, 회귀하는 지도 학습 모델 중 하나

- 결과 모델이 tree 구조를 가지고 있기 때문에 decision tree라는 이름을 가진다. 

- 결정 트리의 기본 아이디어는, leaf node가 가장 섞이지 않은 상태로 완전히 분류되는 것, 즉 복잡성(entropy)이 낮도록 만드는 것이다. 

# 불순도(Impurity)

- 위의 그림처럼 결정 트리에서 분기 기준을 선택하기 위해서는 불순도(Impurity)라는 개념을 사용

- 복잡성을 의미하며, 해당 범주 안에 서로 다른 데이터가 얼마나 섞여 있는지를 뜻한다.
- 다양한 개체들이 섞여 있을수록 불순도가 높아진다.

- 분기 기준 설정 시 현재 노드의 불순도에 비해 자식 노드의 불순도가 감소되도록 설정
- 현재 노드의 불순도와 자식 노드의 불순도 차이를 **Information Gain(정보 획득)**이라 한다.

# 불순도 함수(Gini, Entropy)

불순도를 수치적으로 나타낼 수 있는 대표적인 불순도 함수는 두가지가 있다. 

- 지니 지수(Gini)

- 엔트뢰피 지수(Entropy)


# 정보 획득(Information gain)

- 분기 이전의 불순도와 분기 이후의 불순도의 차이를 정보 획득이라고 한다.

- 불순도가 1인 상태에서 0.7인 상태로 바뀌었다면 정보 획득(information gain)은 0.3이다.

1. root 노드의 불순도 계산
2. 나머지 속성에 대해 분할 후 자식 노드의 불순도 계산
3. 각 속성에 대한 information gain 계산 후 information gain(root 노드와 자식 노드의 불순도 차이)이 최대가 되는 분기 조건을 찾아 분기
4. 모든 leaf 노드의 불순도가 0이 될 때까지 2, 3 반복



# Maximization of information gain





[Logistic Regression](https://itstory1592.tistory.com/8)

[L1 L2 정규화](https://esj205.oopy.io/4b321662-5d02-4559-8677-7e974cf080a8)

[information gain](https://blog.naver.com/samsjang/220976772778)

[의사결정트리](https://blog.naver.com/samsjang/220976772778)

[의사결정트리](https://wooono.tistory.com/104)
