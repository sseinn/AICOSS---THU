# Logistic Regression

사건(Event)이 발생할 가능성을 예측하는 데 사용하는 모델

특정 조건(환경)이 주어졌을 때, 사건이 일어나는지, 일어나지 않는지에 대해 다룸

예측은 0과 1로만 이루어짐

1 : 사건이 일어남

0 : 사건이 일어나지 않음

# Sigmoid Function

로지스틱 회귀 모델에 사용하는 활성화 함수는 시그모이드다. 

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/e8c8df95-b48d-4300-9fd8-d8103a2c64a3)

```
import numpy as np
import matplotlib.pyplot as plt

x = np.arange(-6, 6, 0.1)
phi = 1 / (1 + np.exp(-x))

plt.plot(x, phi)
plt.xlabel('x')
plt.ylabel('phi')

plt.show()
```

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/e28ec939-c08f-4144-9b4c-d5f5c8791218)


시그모이드 함수는 x값이 0일 경우 y값은 0.5라는 결과다. 

x가 0보다 크면 y는 0.5를 기준으로 긍정(사건이 발생) 결과가 나타나고 x가 0보다 작으면 y는 0.5를 기준으로 부정(사건이 발생하지 않는) 결과가 나타난ㄷ. 

y가 0.5인 경우 긍정이라고 보는 경우도 있고, 부저으로 보는 경우도 있다. 


# Linear Regression

선형회귀란 x와 y의 선형적인 관계를 구해 값을 에측하는 것

x는 변수로 어느 값이든 적용 가능

기울기 a와 절편 b는 모르는 상태

-> 기울기와 절편의 값을 알게 된다면, 원하는 x값을 대입했을 때 y값을 얻을 수 있단 말과 동일하다
(y = ax + b)

a : weight

b : bias

# Mean Squared Error - MSE (평균제곱오차)

선형회귀에서 가중치와 바이어스는 MSE를 최소화 하는 방향으로 구한다. 

평균제곱오차 : 데이터의 오차를 제곱해서 데이터의 개수로 나눈 값

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/48b5e5b4-b199-43a9-aa6e-5e0e43a5853a)

그래프에서 데이터의 값은 9이지만, 방정식의 값은 4여서 오차가 9 - 4 = 5여서 오차가 +5가 된다. 

그래프에서 데이터의 값은 3이지만, 방정식의 값은 5여서 오차가 3 - 5 = -2여서 오차가 -2가 된다. 

오차는 음수, 양수 둘다 가능하기 때문에 오차를 제곱하는 것이다. 

그리고 평균을 구하기 때문에 데이터의 개수로 오차의 합을 나눈다. 


# regularization

- 정규화

정규화란 모델 복잡도에 대한 일종의 패널티로, 오버피팅 예방, 일반화(Generalization) 성능을 높이는데 도움을 준다. Regularization 방법으로 L1, L2, Dropout, Early Stopping 등이 있다.

- batch normalization

- weight regularization

- dropout(학습 당시 랜덤으로 절반의 뉴런만 사용하기)

=> weight regularization에 해당

# L1 - Norm, L2 - Norm

모델에서 학습을 진행할 때 학습 데이터에 따라 특정 weight의 값이 커질 수 있다. 이렇게 되면 오버피팅이 일어날 가능성이 아주 높은데, 이를 방지하기 위해 L1, L2 정규화를 사용한다. 

L1 Norm : 벡터 p, q의 각 원소들의 차이의 절대값의 합

L2 Norm : 벡터 p, q의 유클리디안 거리(직선 거리)

# L1 Regularization

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/146bd698-0874-49b4-85d3-6b4045aeac88)


더하기 전 좌측이 일반적인 cost function이고 여기에 가중치 절대값을 더해준다. 

편미분을 하면 w값은 상수값이 되어버리고, 그 부호에 따라 +-가 결정된다. 

가중치가 너무 작은 경우는 상수 값에 의해 weight가 0이 된다. 

=> 결과적으로 몇몇 중요한 가중치들만 남게 됨


# L2 Regularization

![image](https://github.com/sseinn/AICOSS---THU/assets/143159192/39f7a6f3-276e-4f6b-a8fe-a5367f56db3c)

Cost function에 제곱한 가중치 값을 더해줌으로써 편미분을 통해 역전파를 할 때 Cost뿐만 아니라 가중치 또한 줄어드는 방식으로 학습한다. 특정 가중치가 비이상적으로 커지는 상황을 방지하고, weight가 decay 가능하다. 

=> 전체적으로 가중치를 작게하여 과적합을 방

[Logistic Regression](https://itstory1592.tistory.com/8)

[L1 L2 정규화](https://esj205.oopy.io/4b321662-5d02-4559-8677-7e974cf080a8)


