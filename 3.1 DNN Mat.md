```
# Define model
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10)
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits
```

1. init 메서드

   - nn.Flatten() : 입력 이미지를 평평하게 함 -> 이미지를 1차원 벡터로 변환하여 fully connectec 레이어로 전달
  
   - nn.Sequential() : 여러 레이어를 순차적으로 쌓는다. 이 시퀀스는 입력부터 출력까지 각 레이어를 차례로 적용하는데 사용.
  
   - nn.Linear() : 입력 크기가 28*28(=784), 출력 크기가 512인 fully connected 레이어를 정의한다. 



# Training loop

```
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
```

모델을 학습하기 위해서 loss function과 optimizer이 필요하다


- crossEntropyLoss : 크로스 엔트로피는두 확률 분포의 차이를 구하기 위해서 사용된다.
  딥러닝에서는 실제 데이터의 확률 분포와, 학습된 모델이 계산한 확률 분포의 차이를 구하는데 사용된다.


```
# helper function
def train(device, model, dataloader, optimizer, criterion, binary=True, unsqueeze=False):
    model.train() 
    
    running_loss = 0.0
    correct_predictions = 0
    total_predictions = 0
    
    # Iterate over the training dataset
    for inputs, labels in dataloader:
        inputs = inputs.to(device)
        labels = labels.to(device)
        if unsqueeze:
            labels=labels.unsqueeze(1).float()
        # Zero the gradients
        optimizer.zero_grad()
        # Forward pass
        outputs = model(inputs)
        # Compute the loss
        loss = criterion(outputs, labels)
        # Backward pass
        loss.backward()
        # Update the weights
        optimizer.step()
        
        running_loss += loss.item()
         # Compute the predicted labels
        if binary:
            predicted = torch.round(outputs)
        else:
            outputs = torch.softmax(outputs, 1)
            _, predicted = torch.max(outputs, 1)
        # Count correct and total predictions
        total_predictions += labels.size(0)
        correct_predictions += (predicted == labels).sum().item()
        

    # Calculate the average loss for the epoch
    epoch_loss = running_loss / len(dataloader)
    epoch_accuracy = (correct_predictions / total_predictions) * 100
    return epoch_loss, epoch_accuracy
```


이 함수는 주어진 모델과 데이터로더를 사용하여 네트워크를 훈련하는 역할을 합니다.

train(device, model, dataloader, optimizer, criterion, binary=True, unsqueeze=False): 이 함수는 다음과 같은 매개변수를 입력으로 받습니다.

device: 사용할 장치(CPU 또는 GPU)입니다.

model: 훈련할 신경망 모델입니다.

dataloader: 훈련 데이터를 불러오는 데이터로더입니다.

optimizer: 최적화 알고리즘(예: SGD, Adam 등)입니다.

criterion: 손실 함수입니다. 예측과 실제 값 사이의 오차를 계산합니다.

binary: 이진 분류를 위한 플래그로, 기본값은 True입니다. 만약 True로 설정되면 출력을 이진 분류 결과로 처리하고, False로 설정되면 다중 분류를 위한 출력으로 처리합니다.

unsqueeze: 기본값은 False입니다. 만약 True로 설정되면 레이블을 unsqueeze하여 모양을 조정합니다.
함수는 다음을 수행합니다:

model.train(): 모델을 훈련 모드로 설정합니다.

데이터로더를 반복하여 각 훈련 배치에 대해 다음을 수행합니다:

입력과 레이블을 지정된 장치로 이동합니다.

그래디언트를 0으로 초기화합니다.

순전파를 수행하여 출력을 얻습니다.

손실을 계산합니다.

역전파를 수행하여 그래디언트를 계산합니다.

옵티마이저를 사용하여 가중치를 업데이트합니다.

손실을 누적합니다.

정확도를 계산합니다.

모든 배치에 대한 평균 손실과 정확도를 계산하고 반환합니다.

이 함수는 주어진 데이터로 모델을 훈련하고, 각 에포크에서의 손실과 정확도를 반환합니다.
