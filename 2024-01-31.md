# hypterparameters

- describe possible adjustments for any ml-application

- force algorithmic behavior

- have a major impact on model performance and inference quality

- examples:

- learing rate, activation function, momentum, gamma-values, weight decay, kernel size, thresholds, probabilities, ect.

- number of hidden layers, tree depth, number of neurons, number of leaves, ect.

# why should you tune/optimize those parameters?

- improving model quality & performace

- trial & error is time-consuming(expensive)

- automatic documentation of experiments

- improved model insights(white box)

- building trust

# HPO-Evolution

1. not dealing withi iparameter tuning at all

2. manually optimizing parameters

3. utilizing grid search

# gudied optimization

# HPO-Frameworks(optuna)

  텐서플로, 케라스 등등...

# Optuna vocabulary

### Sampler

- strategy to sample set of parameters from given search space

- random search

- grid search

- bayesian optimization search(Tree-structured parzen estimator)

- meta-heuristic search

- custom search

### Pruner

- strategy to omit unpromising sets of parameters

  sampling : 하이퍼 파라미터를 설정하고 학습이 잘 되는지 확인함

# optuna vocab cont.

### study

- defines the experimental setup (e.g.:sampling & pruning strategy, optimization type)

  (study는 여러번의 시도)

### trial

- a single experiment within a study, using a selected set of patameters

### objective function

- provides feedback of the fitness(quality) of a model for a single trial(e.g.: accuracy)

### road map

1. framework basics

2. neural architecture search (NAS)

3. pruning

4. visualization


# Q. Framework 선택 기준이 있냐?

A. 없다. 개인의 선택 

# framework 비교

이 frameworm를 얼마나 더 유지가능 하냐

전반적으로 순위룰 매겨봤는데 optuna가 가장 best였다...



