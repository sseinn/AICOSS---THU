# EDA(Exploratory Data Analysis) 탐색적 데이터 분석

데이터를 분석하고 결과를 내는 과정에 있어서 지속적으로 해당 데이터에 대한 '탐색과 이해'를 기본적으로 가져야 한다. 

데이터를 이해하지 않고 무작정 코드 한 줄 짜는 것보다 데이터를 먼저 이해하고 하는 게 맞다

## How to do EDA

1. raw data의 description, dictionary를 통해서 데이터의 각 column들과 row의 의미를 이해

2. 결측치 처리 및 데이터 필터링 기술

데이터 분석을 본격적으로 들어가기 전, 반드시 데이터에 결측치가 없는지 확인하고, 있다면 제거해야함

분석 시 필요한 데이터가 수치형 데이터(numerical data)인데 범주형(categorical data)으로 되어 있다면 (데이터 타입이 'object'로 뜸) 수치형으로 변환(astype 활용)

데이터에 결측치(NaN, N/A 등)가 있거나, 수치형이어야 하는데 범주형/비수치형(non-numerical data)으로 들어가 있는 데이터로 열심히 데이터프레임 함수를 넣고, 그래프를 그려봤자 원하는 결과를 얻을 수 없다. 

3. 누구나 이해하기 쉬운 시각화를 하는 기술


[EDA](https://jalynne-kim.medium.com/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D-%EA%B8%B0%EC%B4%88-eda%EC%9D%98-%EA%B0%9C%EB%85%90%EA%B3%BC-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D-%EC%9E%98-%ED%95%98%EB%8A%94-%EB%B2%95-a3cac2cc5ebc)



# Feature Engineering

## Feature Engineering 정의

모델 정확도를 높이기 위해 주어진 데이터를 예측 모델의 문제를 잘 표현할 수 있는 features로 변형시키는 과정.

머신러닝 알고리즘을 작동하기 위해 데이터 도메인 지식을 활용해 feature를 만드는 과정이다. 


## Feature Engineering 종류

### 정규분포화

대부분의 통계 기법은 데이터의 분포가 **정규분포**인 가정에서 만들어짐

따라서 특정 데이터의 분포가 정규분포가 아니라면 **정규분포화** 시켜주어야 하는 작업이 필수.

- **정규분포 만드는 방법**

  - log 변환
 
  - sqrt 변환
 
## 이상치(outlier) 제거

이상치는 극단적인 값을 말하는데 마음대로 삭제하는 건 안되지만 무의미하고 분석에 악영향을 준다면 삭제하고 분석하는 것이 맞다. 

보톡 범위를 설정하기 위해 IQR을 사용하는데 제 3사분위수에서 제 1사분위수 값을 제거한다. 


+ 좀 더 자세히

### 이상치(outlier)

모델의 성능을 떨어뜨리는 불필요한 요소이기 때문에 반드시 제거가 필요

- outlier 찾는 방법

1차적으로 EDA 과정에서 그래프를 통해 발견 가능 -> 하지만 이 방법은 소수의 데이터가 평균으로 눈에 띄게 떨어진 경우에만 가능 

-> 따라서 "어디까지가 이상치 데이터다"라고 판단하는 기준이 필요 -> 이 기준이 IQR


### IQR(사분위수 범위, Inter Quantile Range) 

이상치에 레이블을 지정하는 방법

IQR 방식은 사분위(Quantile) 개념으로부터 출발

전체 데이터들을 오름차순으로 정렬하고, 정확히 4등분(25%, 50%, 75%, 100%)으로 나눈다. 

이때 75%, 25% 지점 값의 차이가 IQR이라고 한다. 

[IQR](https://hwi-doc.tistory.com/entry/IQR-%EB%B0%A9%EC%8B%9D%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%9D%B4%EC%83%81%EC%B9%98-%EB%8D%B0%EC%9D%B4%ED%84%B0Outlier-%EC%A0%9C%EA%B1%B0)


## Scailing

대부분의 데이터는 범위, 단위가 다르다. 

그러나 단위가 다르면 직접적인 비교가 불가능하다. (ex 사람 키와 사람 몸무게 비교 -> 단위가 달라서 비교 불가)

이런 문제를 해결하기 위해 scailing을 진행한다. 

- **Scailing** 방법

  - 정규화(Normalization)
 
    
 
  - 표준화(Standardization)
 
    값들이 정규분포를 따른다고 가정하고 값들을 0의 평균 1의 표준편차를 갖도록 변화해주는 것.

    표준화를 해주면 정규화처럼 특성값의 범위가 0~1 사이로 균일하게 바뀌지는 않는다. 


### 정규화 표준화 무엇이 나은가?

어떤 게 더 낫다는 것은 없고 상황에 따라서 다르다. 

따라서 둘 다 진행해보고 어느 것이 더 나은지 비교 후 적용해야한다. 


## Imbalanced Data

데이터가 imbalanced 한 경우 모델이 학습을 제대로 할 수 없다. 

데이터가 한 쪽에만 너무 많을 경우 그 데이터에 대해서 오버피팅이 발생하게 된다. 

따라서 imbalanced data인 경우 다른 대안을 만들어야한다. 


- **Imbalanced Data** 대안

  - 적절한 평가 지표 사용하기
 
    Imbalanced Data는 평가 지표로 **정확도**를 사용하면 많은 클래스의 정확도에서만 높은 정확도를 보이므로 부적절하다.

    따라서 대체 가능한 다른 평가 지표를 사용하는 것이 좋다.

    [Imbalanced Data 평가 지표](https://velog.io/@baeyuna97/ML-%EB%AA%A8%EB%8D%B8-%EC%84%B1%EB%8A%A5-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C)

  - Under-Sampling
 
    balanced dataset으로 resampling 하여 데이터셋을 balanced하게 만드는 것.

    그중 under-sampling은 abundant class의 사이즈를 줄여서 balanced dataset으로 만드는 것이며 이 방법은 데이터의 양이 충분할 때 사용 가능

    rare class의 모든 샘플을 keeping하고 abundant class의 샘플의 숫자를 랜덤으로 선택해서 숫자를 같게 만드는 방법이다. 
    

 
[Feature Engineering](https://velog.io/@baeyuna97/Feature-engineering%EC%9D%B4%EB%9E%80）
