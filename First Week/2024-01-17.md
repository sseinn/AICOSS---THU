# missing values

In Pandas, missing values are internally marked as NaN and can be identified with pd.isnull():

판다스에서 결측값(missing values)은 내부적으로 NaN(Not a Number)으로 표시되며, 이를 확인하기 위해 pd.is null() 함수를 사용할 수 있따. pd.isnull() 함수는 입력과 동일한 모양의 DataFrame 또는 Series를 반환하며 값이 결측된 위치에 True가 표시되고 값이 존재하는 위치에 False가 표시된다. 

```

import pandas as pd

# 결측값이 포함된 예제 DataFrame
data = {'A': [1, 2, None, 4],
        'B': [5, None, 7, 8],
        'C': [9, 10, 11, 12]}

df = pd.DataFrame(data)

# 결측값 확인
missing_values = pd.isnull(df)

print(missing_values)
       A      B      C
0  False  False  False
1  False   True  False
2   True  False  False
3  False  False  False

```


# Heatmap

A heatmap can be very helpful if the number of columns is limited. We therefore look at the first 30 columns below:

heatmap은 열의 수가 제한적인 경우 매우 유용할 수 있다. 

30개의 열을 살펴보면, 히트맵은 데이터의 상대적인 크기를 색상으로 나타내어 시각적으로 표현하는 도구 중 하나다. 그러나 데이터셋이 크고 많은 열을 가지고 있을 때, 모든 열을 히트맵에 표시하면 정보가 혼잡해질 수 있따. 그래서 여기서는 데이터셋의 처음 30개 열만 고려하여 히트맵을 그리는 것이 유용하다는 말이다. 

이렇게 하면 히트맵이 더 간결해지고 이해하기 쉽다. 더 많은 열을 추가할 경우 히트맵의 해석이 어려워질 수 있으므로 필요한 정보에 중점을 두어 데이터를 시각적으로 분석하는 데 도움 된다. 


```
df[cols].isnull().sum()
```

DataFrame인 df에서 특정 열(cols)에 대해 각 열별로 결측값(null 또는 NaN) 개수를 세는 명령어다. 

df : dataframe 객체

[cols] : dataframe에서 특정 열 또는 열들을 선택한다

.isnull() : 각 요소가 결측값인지 아닌지를 나타내는 불리언(boolean) 형식의 dataframe을 생성한다. 

따라서 df[cols].isnull().sum()은 선택한 열들에 대해 각 열별로 결측값의 개수를 나타내는 series를 반환한다. 



# seaborn 이용해 dataframe 결측값 시각화

colours = ['darkblue', 'yellow] : 시각화 할 때 사용할 색상 정의. 여기서 결측값이 있는 부분을 나타내는 darkblue와 결측값이 없는 부분을 나타내는 yellow 선택

plt.figure(figsize=(16,10)) : matplotlib의 figure 함수를 사용하여 그림의 크기를 설정. figsize 매개변수를 통해 가로 16인치, 세로 10인치로 선택

sns.heatmap(df[cols].isnull(), cmap=sns.color_palette(colours)) : seaborn의 heatmap 함수를 사용하여 결측값을 시각화

df[cols].isnull() : 선택한 열들에 대해 결측값 여부를 나타내는 불리언 형식의 dataframe 생성

cmap = sns.color_palette(colours) : 색상 팔레트 설정. 위에서 정의한 darkblue, yellow 사용

결측값이 있는 부분을 파란색, 없는 부분을 노랑색으로 표시


The first 30 columns show a certain symptomatology in the missing values. The first 10,000 entries show missing values in the columns max_floor to state. It is possible that these are older data sets in which these features have not yet been collected.


"첫 30개 열은 결측값에서 특정한 증상을 보입니다. 처음 10,000개의 항목에서는 'max_floor'부터 'state' 열까지의 열들에서 결측값이 나타납니다. 이는 이러한 특징들이 아직 수집되지 않은 오래된 데이터 집합일 수 있다는 가능성이 있습니다."

이 설명에서 언급된 내용은 다음과 같습니다:

처음 30개 열: 데이터 프레임의 처음 30개 열은 어떤 증상 또는 특징을 나타내는데, 이 열들에서 결측값이 특별한 양상을 보이고 있습니다.

처음 10,000개 항목: 데이터 프레임의 처음 10,000개 항목에서는 'max_floor'부터 'state' 열까지의 열들에서 결측값이 나타나고 있습니다. 이로 인해 추론할 수 있는 것은 이러한 열들의 결측값은 해당 열들의 정보가 수집되지 않았을 때 나타날 가능성이 있다는 것입니다.

오래된 데이터 집합: 주어진 데이터가 오래된 데이터 세트일 수 있으며, 특히 처음 10,000개 항목에서는 일부 특징이 수집되지 않아서 결측값이 나타나고 있을 수 있습니다.


# method2 : List with relative frequencies


Since we cannot analyze every column in the data set in a heat map due to the size of the data, we should take a look at which columns contain missing values. We can also specify the relative frequency of the missing values

해당 데이터 세트의 크기가 크기 때문에 히트맵을 통해 모든 열을 분석할 수 없다. 따라서 결측값을 포함하는 열과 해당 결측값의 상대적 빈도를 확인해야한다. 

```
# 새로운 데이터 프레임 생성
df_missing_values = pd.DataFrame(columns=['col', 'percent_missing']).set_index('col')

# 각 열에 대해 결측값의 상대적 비율 계산
for col in df.columns:
    df_missing_values.loc[col] = np.mean(df[col].isnull())

# 결과 출력
df_missing_values.head()
```

이 코드는 각 열별로 결측값의 상대적 비율을 계산하여 데이터 프레임에 저장하는 목적으로 사용된다. 

df_missing_values : 결측값의 상대적 비율을 저장할 새로운 데이터 프레임 생성 

# Method 3: Missing Data Histogram

Finally, we want to analyze the distribution of the missing values within the rows. To do this, we add up the missing values along the column axis:

결측값이 행 내에서 어떻게 분포되어 있는지를 분석하기 위해, 각 행에서 열 방향으로 결측값을 합산합니다. 

```
df.isnull().sum(axis=1).head()
```

데이터프레임 df에서 각 행(axis=1)별로 결측값의 개수를 계산하는 코드다

df.isnull() : 데이터프레임의 각 요소가 결측값인지 아닌지를 나타내는 불리언 형식의 데이터 프레임을 생성한다. True는 결측값을 나타낸다. 

.sum(axis=1) : 각 행(axis=1)에 대해 True(결측값)의 개수를 더하여 해당 행별 결측값의 총 개수를 계산한다.

.head() : 처음 다섯 개의 행만을 출력한다. 

처음 다섯 개의 행에 대해 각 행의 결측값 개수를 나타내는 시리즈를 반환한다. 


# Dealing with missing values

There are various methods for dealing with missing values. It is often unclear which of these is the right one. This often has to be done in close coordination with the business requirements and decided on a case-by-case basis.

결측값(누락된 데이터)을 처리하는 다양한 방법이 있다. 그러나 어떤 방법이 옳은지는 종종 명확하지 않다. 이것은 종종 비지니스 요구사항과 긴밀한 협력을 통해 케이스별로 결정되어야 한다. 

## Method 1: Remove values

We use the df.dropna() method to remove null values from a DataFrame:

```
pd.DataFrame.dropna(axis='index', how='any', thresh=None, subset=None, inplace=False)
```

axis: 0 or 'index' for rows, 1 or 'columns' for columns

how: any removes as soon as at least 1 NaN is present, all only removes if all entries are NaN

thresh: Threshold for the number of non-NaN values

subset: Label of the respective other axis, which are considered

inplace: Inplace operation True or `False

df.dropna() 메서드는 dataframe에서 결측값을 제거하는 데 사용된다. 

axis : 결측값을 제거할 기준이 되는 축을 지정한다. 0 또는 index는 행(로우)을, 1또는 columns은 열을 나타낸다. 

how :  결측값을 제거하는 방식을 나타낸다. any는 해당 행 또는 열에 하나라도 결측값이 있으면 제거하고, all은 모든 항목이 결측값인 경우에만 제거한다. 

tresh : 남겨둘 최소한의 비-결측값 개수를 나타낸다. 이 값보다 적은 비-결측값이 있는 행 또는 열은 제거된다. 

subset : 결측값을 검사할 특정 행 또는 열을 지정할 수 있따. 이것은 검사 대상을 제한하는 데 사용됩니다. 

inplace : True로 설정하면 dataframe이 직접 변경되며 새로운 dataframe을 반환하지 않습니다. false로 설정하면 새로운 dataframe을 반환하고, 기존 dataframe은 변경되지 않습니다. 

이 메서드는 dataframe에 결측값이 있는 행 또는 열을 제거하는 간단하면서 효과적인 방법을 제공한다.


# 1.3.b_Agg_Pivot_1_Ex

# Reshaping theory - Data Cube

The data in a data warehouse is often modeled as a data cube, i.e. it has a multi-dimensional structure with dimensional hierarchies and contains the associated facts as values.


In relational databases, this is often mapped with so-called star schemas. In such a schema, the so-called fact table is located in the center and the dimension tables are located around it. A star schema matching the above data cube as an ER model could look like this:


The following dimensional hierarchies can be recognized:

1. a location hierarchy in branch
From bottom to top, the hierarchy is Designation → City → Region → Country. In the cube such a designation would be OL West, in the city Oldenburg, in the region Lower Saxony in the country Germany.

2. a time hierarchy in time
More precisely, there are two hierarchies here, on the one hand day → week → year and on the other hand day → month → quarter → year (since a calendar week can lie in two different months or even quarters). The date 21.12.1999 from the cube would have exactly this day (or just the 21), the week would be 51/1999 (or just 51), the month 12/1999 (or just 12), the quarter 4/1999 (or just 4) and the year 1999.

3. specific hierarchies in Product
Here, too, there are two hierarchies, namely designation → brand → manufacturer and designation → product group. The latter could possibly be extended by Product main group as the next higher level.

Hierarchies of this kind then form so-called aggregation paths, along which the facts can be aggregated further and further. In the example above, the basic facts, which show how many of a product were sold in a store on a particular day, would then presumably be aggregated with the aggregate function total, for example to an overview of how many products of a product group were sold in a region in a particular year.

Such aggregated data is generally displayed in pivot tables, whereby the data can be further restricted by filter conditions. Sticking with the cube metaphor, pivot tables are mathematically two-dimensional projections (of parts) of the cube.

In addition to the aforementioned concepts that relate to multi-dimensional data storage, there are other concepts that relate to the handling of this data while taking the structure into account, namely multi-dimensional operations.


가장 중요한 다차원 연산에는 다음과 같은 것들이 있습니다:

1. Drill-down(상세화) 및 Roll-up(집계):

- Drill-down(상세화): 계층 구조 내에서 세분화하여 더 상세한 수준으로 이동하는 것입니다. 예를 들어, 연도 → 분기 → 월 → 일로 이동하여 더 상세한 시간 단위로 데이터를 살펴볼 수 있습니다.

- Roll-up(집계): 계층 구조 내에서 집계하여 더 높은 수준으로 이동하는 것입니다. 예를 들어, 일 → 월 → 분기 → 연도로 이동하여 더 높은 수준의 시간 단위로 데이터를 집계할 수 있습니다.

2. Split(분할) 및 Merge(통합):

- Split(분할): 다른 차원(또는 계층)에서 속성을 추가하여 세분화하는 것으로, 다른 차원의 속성을 사용하여 더 상세한 데이터를 얻을 수 있습니다.

- Merge(통합): 속성을 제외함으로써 집계하는 것으로, 다른 차원의 속성을 무시하여 더 높은 수준의 집계된 데이터를 얻을 수 있습니다.

3. Slicing(자르기) 및 Dicing(다이싱):

- Slicing(자르기): 한 속성의 값만을 선택하여 해당 값에 따라 데이터를 제한합니다. SQL의 WHERE 절과 유사합니다.

- Dicing(다이싱): 여러 속성의 값으로 데이터를 제한합니다. 여러 속성을 기준으로 데이터를 자세히 분석합니다.

4. Pivoting(피벗):

이 연산은 데이터 큐브의 관점을 변경하여 이전과는 다른 속성을 서로 비교합니다. 즉, 다른 관점에서 데이터를 살펴볼 수 있습니다.

이러한 연산들은 데이터 큐브의 다양한 관점에서 데이터를 조작하고 분석하는 데 사용됩니다. Drill-down 및 roll-up은 계층적인 시각에서 데이터를 탐색하고 집계하는 데 중요하며, Split 및 Merge는 다른 차원을 통해 데이터를 확장하거나 집계하는 데 사용됩니다. Slicing 및 Dicing은 데이터를 특정 조건에 따라 선택적으로 제한하고 Pivoting은 데이터의 관점을 변경하여 새로운 비교를 제공합니다.


# Pivot tables - implementation in Pandas


1. df.stack() : MultiIndex 객체를 스태킹 하는 작업

2. df.unstack() : MultiIndex 객체를 언스태킹 하는 작업. 즉, 행 인덱스를 열 인덱스로 변환

3. df.pivot() : 데이터를 피벗하는 작업을 수행한다. 집계는 이루어지지 않는다.

4. pd.pivot_tale() : 데이터를 피벗화하는 수행하면서 동시에 집계를 적용할 수 있다. 피벗 테이블을 생성

5. pd.crosstab(): 간단한 크로스탭(교차표)을 생성. 주로 범주형 변수 간의 빈도를 확인하는 데 사용

6. pd.melt() : 데이터를 리버스 피벗화하는 작업을 수행. 즉, 열을 행으로 변환하고 데이터 값을 재구성


